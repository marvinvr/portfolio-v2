---
title: Using Deep Learning to classify Camera-Trap images from the African Jungle
description: Using Computer Vision to tackle the problem of wildlife species classification in Taï National Park using camera trap images.
date: '2023-11-13'
categories:
  - deep-learning
  - computer-vision
published: true
thumbnail: thumbnail.png
header: header.png
---

<script lang="ts">
  import { formula } from "$lib/utils/maths";

  const logLossFormula = formula("loss = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{M} y_{ij} \\log(p_{ij})");
  const f1Formula = formula("F1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}");
  const f1MacroFormula = formula("F1_{macro} = \\frac{1}{M} \\sum_{i=1}^{M} F1_i");
  const f1MicroFormula = formula("F1_{micro} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN}");
  const f1WeightedFormula = formula("F1_{weighted} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{TP_i}{TP_i + FP_i + FN_i}");

  import { image } from "$lib/utils/images";

  const slug = "2023-11-13-wildlife-cameras";
  const exampleImage = image(slug, "examples.png");
  const classDistribution = image(slug, "class_distribution.png");
  const examplesCropped = image(slug, "examples_cropped.png");
  const accuracyTable = image(slug, "accuracy_table.png");
  const accuracyBarplot = image(slug, "accuracy_barplot.png");
  const confusionMatrix = image(slug, "confusion_matrix.png");

</script>


Embarking on an exploration through the African jungle isn't an easy feat—even less so when embarked upon vicariously through the lenses of wildlife cameras. These unblinking eyes capture the untamed life of a world mostly unseen by human eyes, generating an abundance of images rich with potential insights. The question is, how can we harness the power of Deep Learning to sort through this wilderness of pixels and accurately classify each creature that crosses the camera's path? It's within this challenging intersection of technology and nature that our project took root. 

We aimed to develop a computer vision-based solution capable of distinguishing among the myriad of species captured in thousands of photos, automating what has historically been a manual task for wildlife researchers. 

Throughout this blog post, we will delve into my journey, the solutions we discovered, and the wild ride that landed us a spot among the leaders in an international challenge dedicated to wildlife image classification. 

### The Purpose of the Project
Embarking on the quest of digital wildlife exploration, our project aimed at creating an intelligent solution to assist researchers in the rapid and accurate classification of wildlife species from camera trap images in Taï National Park. Our goal was to contribute to the conservation efforts by deploying the powers of Deep Learning to process the voluminous data generated by these traps, a task that once relied heavily on the meticulous eyes of human experts.

The challenge presented an array of seven distinct species that call the park home—birds, civets, duikers, hogs, leopards, other monkeys, and rodents—along with some empty frames devoid of any animals. 

We were provided with a dataset rich with image data coupled with informative attributes intended to train and test our model. With images systematically categorized into training and testing sets, we had a solid foundation to start crafting our model. Crucial here was the condition that our model needed to generalize well to previously unseen contexts since the training and testing sites were completely distinct, challenging us to consider robustness in our solution.

The competition placed strict restrictions that no external dataset beyond what was provided could be utilized, although leveraging publicly available, pre-trained computer vision models was permitted. Our mission was clear: develop a model that could stand the test of new contexts and help transform the way we protect and study the rich biodiversity of Taï National Park.

### Challenges of Wildlife Image Classification
Classifying images from wildlife cameras stationed deep in the African jungle presents a unique set of challenges, a fact illustrated by the diversity in our dataset. As we progressed through the images, we learned that each snapshot was a complex puzzle, waiting to be deciphered by our deep learning algorithms.

![Example Images]({exampleImage})

Firstly, let's consider the lighting conditions. In the dataset, we encountered images similar to the ones above, where the first two are in black and white. The first example image is slightly overexposed, with the animal only partially visible. Whereas the second image is of good quality, but again shows only a fragment of the animal. Such varying exposures significantly complicate the identification process.

Next, we face issues with focus and clarity. Example image three is characterized by only the foreground foliage being in sharp focus, with our subject of interest—the animal—appearing blurred in the background. This poses a particular challenge as the model must learn to recognize animals even in less than ideal sharpness.

Moreover, the composition of the photographs is unpredictable. We have example image five, which captures the animal in motion, only halfway in the frame, suggesting that our model had to be robust against partial views of subjects. Meanwhile, the sixth image shows different lighting from the previous ones. This variation is linked to the time of the snapshot was taken, evidence of diurnal changes in the jungle. While the animal is distinguishable, the image quality is compromised.

Additionally, some images included superimposed metadata as text, a potential source of confusion for the models, which have to distinguish between relevant visual information and superfluous on-screen clutter.

![Class Distribution]({classDistribution})

In addition to the difficult conditions of the image itself, the model was going to have to counter a significant class imbalance in the training data. The hog class, for example, had very few appearances in the dataset, which meant we were working against a natural bias that the model would generate during training.

Our deep learning models had to be adept at navigating through a multitude of complications including imperfect lighting, motion blur, partial animal sightings, diverse focus planes, and extraneous textual data.

## The International Challenge
Within the international arena of AI research, my team entered a prominent competition aimed at pushing the boundaries of computer vision and deep learning as it applies to ecological monitoring. As part of a [DrivenData Challenge](https://www.drivendata.org/competitions/87/competition-image-classification-wildlife-conservation/page/483/) Leveraging a large array of camera trap images from various regions of Taï National Park, the challenge incubated a stimulating environment for innovation and advancement in wildlife image classification.

Participants hailed from different parts of the world, resulting in a rich exchange of ideas and methodologies. The competition was structured with a leaderboard, reflecting the ongoing performance of the different models that teams submitted.

My team worked over the course of several months, tweaking and evolving our approach in response to the shifting dynamics on the leaderboard. This exercise wasn't solely a matter of academic interest, but a real-world application with the potential to significantly impact conservation strategies and biodiversity research. 

Landing in the third spot, we maintained a position within the upper end of the leaderboard, a testament to our dedication and the effectiveness of our solutions. Our placement was particularly rewarding given the challenge's complexity and the high caliber of our global competitors.

## Applied Technologies
In our quest to classify wildlife images from the African jungle, our team employed a set of powerful hardware and software tools to enable efficient experimentation and result analysis.

We chose **Python 3.10** for its rich ecosystem of libraries and straightforward syntax, which is particularly well-suited for machine learning and data processing tasks. The choice of Python also streamlined the integration with other tools.

To manage our experiments, we relied on **Weights & Biases**, an essential tool for tracking metrics, outcomes, and predictions. It provided us with a clear overview of our models' performance in relation to their hyperparameters, effectively mapping out the path from input to result, thereby enabling us to pinpoint which adjustments led to improvements in performance.

When it came to machine learning frameworks, we integrated **Pytorch Lightning** into our workflow. Pytorch Lightning offers a class-based structure, bringing order and clarity to our deep learning projects. It simplifies the integration with Weights & Biases and significantly reduces boilerplate code, making our codebase cleaner and more maintainable.

These technologies formed a robust foundation for our deep learning project, streamlining our processes and allowing us to focus on the real problem.

## Data Preprocessing
Preparing our data for the deep learning task was an early critical step in our project. To mitigate the risk of overfitting our models to the training data, we implemented a transformation strategy as part of our regularization approach. Every iteration during training, these transformations would slightly alter the data and, consequently, the data's gradient, enhancing the model's ability to generalize to new, unseen images.

```python
def _get_augmented_transform(image_size: int):
    return transforms.Compose(
        [
            transforms.Resize((image_size, image_size)),
            transforms.RandomHorizontalFlip(),
            transforms.ColorJitter(
              brightness=0, contrast=0, saturation=0, hue=0
              ),
            transforms.Grayscale(3),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ]
    )
```

In the training phase, our function `_get_augmented_transform` applied a series of image processing operations, known as transformations, to each image. The sequence in which these transformations were listed in the function defined the order of their application, which included:

- **Resizing**: We first adjusted the size of every image to a specified `image_size` to ensure a consistent size across all images—a crucial requirement for most computer vision models which need fixed input dimensions.

- **Random Horizontal Flip**: This transformation randomly flipped images horizontally, aiming to improve the model's robustness to the varied horizontal orientation of animals in the images.

- **Color Jitter**: Even though we set the parameters for brightness, contrast, saturation, and hue to zero, this transformation usually introduces random color variations to the image. However, since the African jungle images were taken under various lighting conditions, we chose not to manipulate the image colors further.

- **Conversion to Tensor (ToTensor)**: This converted the image from its original format (PIL or NumPy) into a PyTorch Tensor, making it compatible with PyTorch models.

- **Normalization**: By subtracting the mean and dividing by the standard deviation, the images were normalized to improve numerical stability and ensure feature scales were consistent. However, specific mean and std values adapted to our dataset were not provided here.

These transformations exemplify data augmentation techniques used to enrich training data diversity and quantity, bolstering the model's robustness, and curtailing overfitting.

```python
def _get_transform(image_size: int):
    return transforms.Compose(
        [
            transforms.Resize((image_size, image_size)),
            transforms.Grayscale(3),
            transforms.ToTensor(),
            transforms.Normalize(mean, std),
        ]
    )
```

For validation and test data, we employed a streamlined version of these transformations. Since overfitting is not a concern with data not directly learned by the model, and our aim was to predict these images as accurately as possible, only **Resize**, **ToTensor**, and **Normalize** were used. This approach kept the validation and test data consistent with the training data concerning size and color space, ensuring cohesion across the entire dataset.

## Deep Learning Models
We knew that the right model could mean the difference between a blurry guess and a sharp prediction. Recognized for their ability to make sense of complex and high-dimensional data, deep learning models have excelled in computer vision tasks, making them ideal candidates for our challenge.

In selecting a modeling approach, we capitalized on the power of transfer learning. This technique allowed us to take advantage of pre-trained models that had already learned robust visual representations from large, diverse datasets such as ImageNet. By retraining or fine-tuning these models on our specific dataset, we could reach higher accuracy levels more quickly than training from scratch.

On the one side, deeper and more complex models can encompass a wide range of features and interactions, but could potentially be more prone to overfitting without sufficient regularization. On the other hand, simpler models may miss out on crucial subtle patterns in the data.

The journey to find the optimal model was a balancing act of precision, efficiency, and the ever-present limitations of computational resources.

### The Models We Experimented With
We experimented with a range of architectures, each with its own merits and complexities:

**ResNet-50**, known for its residual learning framework which facilitates deeper network training by allowing feature reuse through identity skip connections, offered a promising foundation and robust performance on image recognition tasks.

**EfficientNetv2** caught our attention with its scalability and balance between depth, width, and resolution, bolstered by novel training strategies and architecture optimization that could handle our diverse and challenging dataset efficiently.

We also leveraged the **DenseNet-161** architecture, distinguished by its dense connectivity pattern where each layer receives additional inputs from all preceding layers and passes on its own feature-maps to all subsequent layers, aiming to strengthen feature propagation and encourage feature reuse.

**VGG-19**, with its simplicity and depth which had shown considerable success in large-scale image recognition challenges, served as a baseline to measure the benefits of more complex architectures, despite its relatively higher computational cost.

**Inception V3**’s appeal lay in its asymmetric design, which allowed it to learn at multiple scales and apply convolutions of different sizes concurrently.

Lastly, the **Vision Transformer (ViT)** distinguished itself from its convolutional counterparts. As an attention-based model initially developed for natural language processing, ViT utilizes a self-attention mechanism to weigh the importance of different parts of the input data, a novel approach in our suite of models that holds promise for the intricacies of wildlife image classification. 

Our model experimentation aimed not only to ascertain the most effective architecture, but also to glean insights into the unique demands of the wildlife classification domain. Each model was a piece in the puzzle, contributing unique insights that would lead us closer to the apex of accurate and reliable image classification in the wild.

### Selecting the Best Performing Model
Selecting the best-performing model from our experiments involved a rigorous evaluation process that incorporated both the Log Loss and an expanded set of F1 scores. 

We primarily utilized the Log Loss, or logarithmic loss function, a staple evaluation metric in many neural network frameworks, for its ability to quantify the accuracy of a classifier by penalizing false classifications. Its formulation emphasizes not just whether predictions are wrong, but also the extent of their deviation from the truth. 

{@html logLossFormula}

The Log Loss is particularly expressive when dealing with probabilistic predictions of multiple classes, and for our project, it served as a direct link to the evaluation criteria used in the challenge for which we developed our models. It works by taking the predicted probability of the true class and applying the logarithm, reverting the negative sign since logarithms of probabilities yield negative values. The sum of these is then averaged across all observations, giving us a sense of model performance on an intuitive scale—the lower the Log Loss, the better.

{@html f1Formula}

In addition to Log Loss, we incorporated the family of F1 scores. The F1 score captures the balance between precision and recall, serving as a more tangible metric of a model's performance, especially for stakeholders not intimately familiar with the nuances of machine learning. For our multi-class problem, we looked at three common variants: Micro-F1, Macro-F1, and Weighted-F1.

{@html f1MicroFormula}

Micro-F1 aggregates performance across all classes, presenting a global view on true positives, false positives, and false negatives to calculate precision, recall, and thus F1. This approach is useful when there's a class imbalance, as it ensures that all classes contribute equally to the final score.

{@html f1MacroFormula}

Macro-F1 instead considers each class independently when calculating precision and recall, averaging out the individual F1 scores. It gives equal weight to each class, regardless of their variability in size within the dataset, making it possible for minority classes to have as much influence on the score as the majority ones.

{@html f1WeightedFormula}

The Weighted-F1 score refines the Macro-F1 approach by adjusting the influence of each class in accordance with the number of observations it includes. This offers a more nuanced evaluation that favors larger classes and is of particular benefit where data distributions are unequal across classes.

By considering both Log Loss and the family of F1 scores, we achieved a comprehensive understanding of our models' capabilities. This combined approach not only identified the best-performing model based on predictive confidence but also highlighted its balance of precision and recall across the varied classes. The models that surfaced as front-runners were those that minimized Log Loss while maintaining high F1 scores, reflecting both accuracy and reliability in their predictions.

### Discouraging Initial Results
Despite meticulous optimization across all our models, we hit a plateau with a validation loss plateauing at around `1.25` to `1.30`. No matter the tweaks we made, we couldn't significantly push past this barrier. This realization led us to acknowledge that minor tuning of hyperparameters was not going to be sufficient. We recognized the need for more drastic changes in our approach to both the models and our methodology overall.

## Cropping using Megadetector
In our quest to streamline the classification of wildlife captured in camera trap images, we turned to the use of an additional model to crop subjects within the pictures before classification. Among the options like **Yolo**, we chose **Megadetector** for its prior training on camera trap data, which, we believed, would yield superior results.

We performed a one-off cropping of all our images using **Megadetector**, conveniently found in Microsoft's [CameraTraps GitHub repository](https://github.com/microsoft/CameraTraps). Following the cropping process, we stored the images in a zip file on our GitLab repository. This approach was strategic; cropping is time-consuming, and we wanted to avoid this computation in every training session. Our images were cropped at varying certainty thresholds of `5`, `10`, `20`, `35`, and `50`.

![Examples of Cropped Images]({examplesCropped})

When we observe examples of cropped images, we were certain, that an improvement was imminent. Megadetector has proved adept at focusing on the relevant subject in the frame, removing the bulk of unnecessary data. This fine-tuning has a significant upside – it allows our models to concentrate on pertinent details, which we anticipate will lead to a marked leap in our classification model's performance.

## Results and Analysis
In our evaluation, we relied on a suite of metrics to gauge the models' performance: class-wise accuracy, weighted, macro, and micro versions of the F1-score, and the Log-Loss. These metrics painted a full picture of how well the models were doing.

![Accuracy Table]({accuracyTable})

The accuracy table for each model across various classes highlighted their strengths and weaknesses. It provided significant insight which could guide the development of new models and refinement of existing ones.

![Accuracy Barplot]({accuracyBarplot})

The accuracy barplot offered a straightforward visual comparison of model accuracies by class. With color-coded bars representing different classes, we could quickly discern patterns and disparities in model performance.

Following the cropping optimization using Megadetector, the **Vision Transformer (ViT)** emerged as a top-performing model. It achieved an F1-Weighted of `0.7569`, an F1-Macro of `0.6678`, and an F1-Micro of `0.7562`, signaling consistent performance across classes. Its Log-Loss of `0.7816` also indicated a solid classification capacity.

Particularly impressive was the ViT model's capability to classify `antelope_duiker`, `civet_genet`, and `monkey_prosimian` with accuracies surpassing `80%`, indicating excellent adaptability on our dataset. The lowest accuracy across models was observed for the 'hog' class, suggesting this might inherently be a challenging class to distinguish, possibly due to its limited representation in the dataset.

![Confusion Matrix]({confusionMatrix})

The confusion matrices for each model served as a powerful tool, giving a detailed view of their performance. Correct predictions lined the main diagonal, while off-diagonal entries represented misclassifications.

The matrices allowed us to identify which classes were reliably recognized and pinpoint areas needing improvement. Understanding these intricacies was vital in planning targeted optimizations for future endeavors and clarifying class relationships to anticipate possible classification confusions.

We can see, that especially before cropping the images using Megadetector, most models had severe problems correctly classifying the images. The line becomes drastically more diagonal when we apply the cropping model before classifying.

## Summary of Findings
After thorough analysis and comparison of the different models, it is clear that cropping the images before classification significantly influenced model performance. The enhanced performance of post-cropping underscores the importance of this preprocessing step to eliminate irrelevant image information, thereby sharpening the focus on the most pertinent features. This highlights the necessity of continually optimizing and adjusting data preprocessing strategies to achieve the best possible results.

All models exhibited commendable performance in identifying certain classes, though there is room for improvement in others, especially noticeable in the 'hog' category, where all models showed low accuracy. A deeper examination of the data, specifically for the `hog` class, could potentially enhance classification. Given that this class is the least represented in the dataset, a possible avenue to explore includes collecting more data for this class, employing additional data augmentation techniques like copying existing data with slight modifications to prevent overfitting.

Overall, we are highly satisfied with our models' outcomes. All models performed adequately, notably in the classification of the `monkey_prosimian` category. The Vision Transformer (ViT) model, in particular, stands out in terms of accuracy, as well as F1 scores and Log-Loss. This performance demonstrates the effectiveness of the Transformer architecture in image classification, confirming its strong capabilities in processing image information and showcasing the potential power of Transformer-based models for image classification tasks.

These results emphasize the success of our work and the high quality of our models. The insights obtained from this project not only advance my understanding of deep learning applications in wildlife image classification but also set a precedent for future endeavors in this domain.